{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week's lab has been adapted from CCI lecturer Terence Broad's [AI for Media materials](https://git.arts.ac.uk/tbroad/AI-4-Media-23-24/). \n",
    "\n",
    "# Prepare dataset\n",
    "\n",
    "In this notebook we are going to prepare our dataset into a binary which is a list of our pre-calculated tokens. (In Natural Language Processing, a \"token\" is the basic unit of text used for analysis or generation; a token could be a single character, a word, or a part of a word. [Read more here](https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/).)\n",
    "\n",
    "Doing this step makes training more efficient as we are not doing that conversion at each training step.\n",
    "\n",
    "First lets do some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def merge_txt_files(input_folder, output_file):\n",
    "    # 获取文件夹中所有txt文件的文件名\n",
    "    txt_files = [f for f in os.listdir(input_folder) if f.endswith('.txt')]\n",
    "    \n",
    "    # 打开输出文件\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        # 遍历每个txt文件并将其内容写入输出文件\n",
    "        for txt_file in txt_files:\n",
    "            with open(os.path.join(input_folder, txt_file), 'r', encoding='utf-8') as infile:\n",
    "                outfile.write(infile.read())\n",
    "                outfile.write(\"\\n\")  # 可选：添加换行符以分隔文件内容\n",
    "\n",
    "# 使用示例\n",
    "input_folder = '/Users/guling/Desktop/coding_final/vosk-api-master/test'  # 替换为包含txt文件的文件夹路径\n",
    "output_file = 'merged_output.txt'  # 合并后的输出文件路径\n",
    "merge_txt_files(input_folder, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from nanoGPT.data import prepare_dataset_as_chars, prepare_dataset_gpt_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure dataset paths\n",
    "\n",
    "By default we are going to load the complete works of shakespeare here, but you can load in whatever text file you want here as your training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = '/Users/guling/Desktop/coding_final/class-5/merged_output.txt'\n",
    "dataset_path ='/Users/guling/Desktop/coding_final/class-5/data/class-datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset as characters\n",
    "\n",
    "Here we will tokenise the dataset as characters. This is for training simple models like babyGPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 111,487\n",
      "all the unique characters: \n",
      " 'abcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 29\n",
      "train has 100,338 tokens\n",
      "val has 11,149 tokens\n"
     ]
    }
   ],
   "source": [
    "prepare_dataset_as_chars(text_path, os.path.join(dataset_path, 'TED_chars'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset as GPT2 Tokens\n",
    "\n",
    "Here we will tokenise the dataset using OpenAIs tokeniser for GPT2. This is for fine-tuning pre-trained GPT2 models. If you are interested in how they come about these tokenizers, see [Andrej Karpathy's minibpe repo](https://github.com/karpathy/minbpe):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (3638905602.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    prepare_dataset_gpt_tokenizer(text_path, os.path.join(dataset_path, TED_tokens'))\u001b[0m\n\u001b[0m                                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "prepare_dataset_gpt_tokenizer(text_path, os.path.join(dataset_path, TED_tokens'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
